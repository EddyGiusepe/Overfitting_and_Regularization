# Overfitting and Regularization

Aumentar a quantidade de dados de treinamento é uma maneira de reduzir o `overfitting`. Mas existem outras maneiras de reduzir a extensão de ocorrência do overfitting? Uma abordagem possível é reduzir o tamanho da nossa rede. No entanto, redes grandes têm o potencial de serem mais poderosas do que redes pequenas e essa é uma opção que só adotaríamos com relutância. 


Neste tutorial estudaremos as seguintes técnicas (abaixo) que nos ajudará a reduzir o `Overfitting`:


1.- Regularização dos pesos (Weight Regularization):

Esta técnica consiste em um ajuste na `Função de Perda` (Loss Function) com o objetivo de penalizar pesos muito altos.

    * Regularização L1 (Lasso Regularization)


    * Regularização L2 (Ridge Regression)

2.- Early Stopping

3.- Dropout

4.- Data Augmentation

5.- `Reduzir o tamanho da nossa Rede Neural` 

![image](https://user-images.githubusercontent.com/69597971/183274556-35d12d0b-a7c1-4965-b8ed-6da659543d13.png)










Links de estudo:

* [Data Science Academy: Overfitting e Regularização](https://www.deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/)




Thanks God!


