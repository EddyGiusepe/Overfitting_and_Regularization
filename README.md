# Overfitting and Regularization

Aumentar a quantidade de dados de treinamento é uma maneira de reduzir o `overfitting`. Mas existem outras maneiras de reduzir a extensão de ocorrência do overfitting? Uma abordagem possível é reduzir o tamanho da nossa rede. No entanto, redes grandes têm o potencial de serem mais poderosas do que redes pequenas e essa é uma opção que só adotaríamos com relutância. 


Neste tutorial estudaremos as seguintes técnicas (abaixo) que nos ajudará a reduzir o `Overfitting`:


1.- Regularização dos pesos (Weight Regularization):

Esta técnica consiste em um ajuste na `Função de Perda` (Loss Function) com o objetivo de penalizar pesos muito altos.

    * Regularização L1 (Lasso Regularization)


    * Regularização L2 (Ridge Regression)

2.- Early Stopping

3.- Dropout

4.- Data Augmentation

5.- `Reduzir o tamanho da nossa Rede Neural` 

![image](https://user-images.githubusercontent.com/69597971/183274556-35d12d0b-a7c1-4965-b8ed-6da659543d13.png)


![image](https://user-images.githubusercontent.com/69597971/183275752-f7e5352f-fe5b-46fb-8057-1759cce304db.png)



![image](https://user-images.githubusercontent.com/69597971/183275748-b9b1bb63-e71b-438e-bb91-1fbe72078408.png)





Links de estudo:

* [Data Science Academy: Overfitting e Regularização](https://www.deeplearningbook.com.br/overfitting-e-regularizacao-parte-2/)

* [Why "Early-stopping works as Regularization?"](https://medium.com/@rahuljain13101999/why-early-stopping-works-as-regularization-b9f0a6c2772)

* [Kaggle: 7 Simple Technique to Prevent Overfitting](https://www.kaggle.com/getting-started/157623)


Thanks God!


